# ğŸ§  Enterprise Deep-Research Agent

An automated Research-as-a-Service (RaaS) platform that leverages **Microsoft AutoGen**, **FastAPI**, and **Streamlit** to perform intelligent document analysis. This system uses a multi-agent orchestration pattern to ingest PDF documents, index them into a vector database, and perform context-aware research.

---

# ğŸ¤– Project Demo
Here is the autonomous agent in action:

![Watch the demo](./assets/demo.mp4)

## ğŸš€ Key Features

* **Multi-Agent Orchestration**: Utilizes Microsoft AutoGen to manage a "Researcher" assistant and a "UserProxy" for tool execution.
* **Production-Ready RAG**: Full Retrieval-Augmented Generation pipeline using LangChain and ChromaDB.
* **Decoupled Architecture**: Clean separation between the AI logic (Backend API) and the User Experience (Streamlit).
* **Fully Dockerized**: Seamless deployment using Docker Compose for both backend and frontend services.
* **Document Lifecycle Management**: Upload, process, and index PDFs directly through the UI.

---

## ğŸ› ï¸ Technical Stack

| Component | Technology |
| --- | --- |
| **Agent Framework** | Microsoft AutoGen |
| **Backend API** | FastAPI, Uvicorn |
| **Frontend UI** | Streamlit |
| **Vector Database** | ChromaDB |
| **Orchestration** | LangChain |
| **Deployment** | Docker, Docker Compose |
| **Language** | Python 3.12 |

---

## ğŸ“ Project Structure

```text
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api.py          # FastAPI application & REST endpoints
â”‚   â”œâ”€â”€ main.py         # Agent orchestration logic
â”‚   â”œâ”€â”€ agents.py       # AutoGen agent & tool definitions
â”‚   â”œâ”€â”€ ingest.py       # Document processing pipeline
â”‚   â”œâ”€â”€ config.py       # Pydantic settings management
â”‚   â””â”€â”€ app.py          # Streamlit dashboard
â”œâ”€â”€ data/               # Source PDFs for ingestion
â”œâ”€â”€ chroma_db/          # Persistent vector storage
â”œâ”€â”€ docker-compose.yml  # Multi-container configuration
â””â”€â”€ Dockerfile          # Shared environment for API & UI

```

---

## âš™ï¸ Setup & Installation

### 1. Prerequisites

* Docker and Docker Compose installed.
* A GitHub Personal Access Token (for the Azure/GitHub Models inference).

### 2. Environment Configuration

Create a `.env` file in the root directory:

```env
GITHUB_TOKEN=your_token_here
API_URL=http://api:8000

```

### 3. Deployment

Spin up the entire stack with a single command:

```bash
docker-compose up --build

```

* **Streamlit UI**: `http://localhost:8501`
* **FastAPI Docs**: `http://localhost:8000/docs`

---

## ğŸ¤– How It Works

1. **Ingestion**: Documents are placed in the `/data` folder. The `ingest.py` script splits the text into chunks and generates vector embeddings.
2. **Research Query**: When a user submits a question, the **UserProxy** agent triggers the `query_knowledge_base` tool.
3. **Synthesis**: The **Researcher** agent receives the retrieved document chunks, analyzes the content, and synthesizes a comprehensive answer.
4. **Termination**: The agents continue the dialogue until a final answer is reached, marked by the `TERMINATE` signal.

---

## ğŸ“ˆ Future Roadmap

* [ ] Implement **HuggingFace** local embeddings to replace FakeEmbeddings.
* [ ] Add **asynchronous support** for long-running research tasks.
* [ ] Integrate **web search** tools to supplement internal document knowledge.
* [ ] Support for **Excel and Markdown** file ingestion.


